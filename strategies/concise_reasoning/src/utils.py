import os
import numpy as np
from collections import defaultdict 
from datetime import datetime

from .training_utils import parse_answer
from .math_parser import compare_answers
from .math_parser import extract_math_answer as extract_math_answer_new
from .math_parser_old import extract_math_answer as extract_math_answer_old


# Root directory for datasets
DATASET_ROOT = 'data'


def get_config_dir(args) -> str:
    """
    Generates directory path based on the provided arguments

    Parameters:
        args (argparse.Namespace): Parsed command-line arguments.
    
    Returns:
        str: Directory path
    """
    if args.prompt_system_key is not None:
        key = args.prompt_system_key
    else:
        key = args.prompt_system
    return f"{args.save_path}/{args.dataset}/{args.model_name}/{args.prompt}/{key}/{args.max_new_tokens}/{args.temperature}/{args.top_k}/{args.top_p}/{args.num_diverse_path}/{args.batch_size}"


def get_train_dir(args) -> str:
    """
    Generates directory path based on the provided arguments

    Parameters:
        args (argparse.Namespace): Parsed command-line arguments.

    Returns:
        str: Directory path
    """
    return f"{args.dataset}/{args.model_name}/{args.type}/{args.percentage}/{args.grad_steps}/{args.seed}/{args.lr}/{args.batch_size}"


def convert_to_json(datasets, outputs, token_counts, output_dir, model_name=None, chunk_index=0) -> str:
    """
    Convert chunk datasets and outputs to JSON.

    Parameters:
        datasets (Dataset): original dataset.
        outputs (list): output generated by the model.
        token_counts (list): number of tokens for each generated output.
        output_dir (str): path where to save.
        model_name (str, optional): name of the model to determine which parser to use
        chunk_index (int): chunk index
    
    Returns:
        str: Path to the saved output file.
    """
    # Normalize model name if provided
    if model_name:
        model_name = normalize_model_name(model_name)
    
    # Flatten the nested list
    outputs = [item for sublist in outputs for item in sublist]

    # Ensure token_counts matches the length of outputs
    if len(token_counts) != len(outputs):
        raise ValueError("The number of token counts does not match the number of outputs.")

    # Parse answers
    answers = []
    for i in range(len(outputs)):
        dataset_type =  datasets[i]['dataset']
        if dataset_type == 'math':
            question = datasets[i]['input']
            rationale = outputs[i]
            # Use appropriate parser based on model name
            if model_name and is_new_parser_model(model_name):
                answer = str(extract_math_answer_new(question, rationale))
            else:
                answer = str(extract_math_answer_old(question, rationale))
        else:
            answer = parse_answer(outputs[i], dataset_type)
        answers.append(answer)
    
    datasets = datasets.add_column("rationale", outputs)
    datasets = datasets.add_column("answer", answers)
    datasets = datasets.add_column("token_count", token_counts)
    
    # Create filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(output_dir, f"output_{chunk_index}_{timestamp}.json")
    
    datasets.to_json(output_file)

    print(f"Chunk {chunk_index} saved to {output_file}")
    
    return output_file


def filter_rationales(data, max_tokens=512, only_correct=True, dataset_type=None, logger=None):
    """
    Filter rationales based on correctness, duplicates, and token count.
    
    Parameters:
    -----------
    data : list
        List of dictionaries containing the data.
    max_tokens : int
        Maximum number of tokens to include (default: 512)
    only_correct : bool
        If True, only return correct answers where answer matches label (default: True)
    
    Returns:
    --------
    list : Filtered rationales with duplicates removed
    """
    
    def is_correct(item):
        if dataset_type == 'math':
            return compare_answers(item['input'], item['label'], item['answer'])
        return item['answer'] == item['label']
    
    total_correct = sum(1 for item in data if is_correct(item))
    
    # Analyze initial data
    if logger:
        total_questions = len(set(item['input'] for item in data))
        total_responses = len(data)
        
        # Calculate accuracy statistics
        match_counts = defaultdict(lambda: {'total': 0, 'correct': 0})
        
        for item in data:
            question = item['input']
            match_counts[question]['total'] += 1
            if is_correct(item):
                match_counts[question]['correct'] += 1
        
        # Derived statistics
        accuracy = (total_correct / total_responses) * 100
        
        correct_per_question = [d['correct'] for d in match_counts.values()]
        mean_correct = np.mean(correct_per_question)
        median_correct = np.median(correct_per_question)
        std_correct = np.std(correct_per_question)
        
        no_correct = sum(1 for d in match_counts.values() if d['correct'] == 0)
        all_correct = sum(1 for d in match_counts.values() if d['correct'] == d['total'])
        
        # Log initial statistics
        logger.info(f"Generated dataset statistics:")
        logger.info(f"Number of questions: {total_questions}")
        logger.info(f"Total responses: {total_responses}")
        logger.info(f"Total correct solutions: {total_correct}")
        logger.info(f"Overall accuracy: {accuracy:.2f}%")
        logger.info(f"Mean correct solution per question: {mean_correct:.2f}")
        logger.info(f"Median correct solution per question: {median_correct:.2f}")
        logger.info(f"Standard deviation of correct solution: {std_correct:.2f}")
        logger.info(f"Questions with no correct solutions: {no_correct} ({no_correct/total_questions*100:.2f}%)")
        logger.info(f"Questions with all correct solutions: {all_correct} ({all_correct/total_questions*100:.2f}%)")
        
    incorrect_count = len(data) - total_correct
    
    # Step 1: Filter by correctness
    if only_correct:
        correctness_filtered = [item for item in data if is_correct(item)]
    else:
        correctness_filtered = data
        
    # Step 2: Remove duplicates efficiently
    seen_rationales = set()
    unique_data = []
    duplicates_removed = 0
    
    for item in correctness_filtered:
        if item['rationale'] not in seen_rationales:
            seen_rationales.add(item['rationale'])
            unique_data.append(item)
        else:
            duplicates_removed += 1
                
    # Step 3: Filter by token count
    removed_by_tokens = len([item for item in unique_data if item['token_count'] >= max_tokens])
    filtered_rationales = [item for item in unique_data if item['token_count'] < max_tokens]
    
    if logger:
        logger.info("Filtering steps:")
        logger.info(f"Step 1: All incorrect rationales - {incorrect_count} ({incorrect_count/total_responses*100:.2f}% of total)")
        logger.info(f"Step 2: Remove duplicates - {duplicates_removed} ({duplicates_removed/total_responses*100:.2f}% of total)")
        logger.info(f"Step 3: Exceeded the token limit - {removed_by_tokens} ({removed_by_tokens/total_responses*100:.2f} % of total)")
        logger.info(f"This procedure gives {len(filtered_rationales)} correct rationales for training")
        
        if filtered_rationales:
            # Calculate token statistics
            token_counts = [r['token_count'] for r in filtered_rationales]
            unique_questions = len({item['input'] for item in filtered_rationales})
            
            logger.info("Token statistics")
            logger.info(f"Number of unique questions: {unique_questions}")
            logger.info(f"Mean token count: {np.mean(token_counts):.2f}")
            logger.info(f"Median token count: {np.median(token_counts):.2f}")
            logger.info(f"Standard deviation of token counts: {np.std(token_counts):.2f}")
            logger.info(f"Minimum token count: {min(token_counts)}")
            logger.info(f"Maximum token count: {max(token_counts)}")
    
    return filtered_rationales


def normalize_model_name(model_name: str) -> str:
    """
    Normalizes and validates model names to ensure consistent formatting.
    
    Parameters:
        model_name (str): Raw model name string
        
    Returns:
        str: Normalized model name
        
    Raises:
        ValueError: If model name is invalid or unsupported
    """
    if not model_name:
        raise ValueError("Model name cannot be empty")
    
    # Convert to lowercase for consistency
    name = model_name.lower().strip()
    
    # List of supported model families
    SUPPORTED_FAMILIES = {
        'llama': ['3.1', '3.2'],
        'qwen2.5': [],  # No version validation needed
        'gemma': ['2'],
        'deepseek': []  # No version validation needed
    }
    
    # Extract model family
    model_family = None
    for family in SUPPORTED_FAMILIES.keys():
        if name.startswith(family):
            model_family = family
            break
    
    if not model_family:
        raise ValueError(f"Unsupported model family. Model name must start with one of: {list(SUPPORTED_FAMILIES.keys())}")
    
    # Validate instruction format
    if not any(suffix in name for suffix in ['instruct', 'it']):
        raise ValueError("Model must be an instruction-tuned variant (ending in 'instruct' or 'it')")
    
    # Version validation for specific families
    if SUPPORTED_FAMILIES[model_family]:  # If there are versions to validate
        valid_versions = SUPPORTED_FAMILIES[model_family]
        if not any(version in name for version in valid_versions):
            raise ValueError(f"Invalid version for {model_family}. Supported versions: {valid_versions}")
    
    return name


def is_new_parser_model(model_name: str) -> bool:
    """
    Determines if a model should use the new math parser.
    Only Gemma models and Qwen 2.5 3B model (non-math variant) should use the new parser.
    
    Parameters:
        model_name (str): Normalized model name
        
    Returns:
        bool: True if model should use new parser, False otherwise
    """
    model_name = model_name.lower()
    
    # Gemma models always use new parser
    if model_name.startswith('gemma'):
        return True
        
    # Only Qwen 2.5 3B (non-math variant) uses new parser
    if model_name.startswith('qwen2.5'):
        # Check it's the 3B model and not the math variant
        return '3b-instruct' in model_name and 'math' not in model_name
        
    return False