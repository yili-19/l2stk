import os
import re
import json
import glob
import argparse
from datasets import load_dataset
from datasets import DatasetDict, Dataset
from collections import defaultdict


from .math_parser import extract_math_answer as extract_math_answer_new
from .math_parser_old import extract_math_answer as extract_math_answer_old
from .utils import normalize_model_name, is_new_parser_model


# Root directory for datasets
DATASET_ROOT = 'data'

class DatasetLoader:
    """
    Class for loading and processing datasets.
    """

    def __init__(self, dataset_name, dataset_version, has_valid, split_map) -> None:
        """
        Initialize DatasetLoader object

        Parameters:
            dataset_name (str): Name of the dataset
            dataset_version (str): Version of the dataset
            has_valid (bool): Indicates if the dataset has a validation set.
            split_map (dict): Mapping of splits to train and test
        """
        self.data_root = DATASET_ROOT
        self.dataset_name = dataset_name
        self.dataset_version = dataset_version
        self.has_valid = has_valid
        self.split_map = split_map
        self.logger = None

        # Check if split_map is not None
        assert self.split_map is not None
    
    def set_logger(self, logger):
        """Set logger for the loader"""
        self.logger = logger

    def load_from_source(self) -> dict:
        """
        Load dataset from the Hugging Face dataset library.

        Returns:
            dict: Loaded datasets.
        """
        # Configure the full path
        if self.dataset_name == 'mmlu-pro':
            dataset_name = 'TIGER-Lab/MMLU-Pro'
        else:
            # Use the original dataset name
            dataset_name = self.dataset_name
        
        # Load dataset with specifed version, if not provided, load the latest version
        datasets = load_dataset(dataset_name, self.dataset_version) if self.dataset_version else load_dataset(dataset_name)
        return datasets
    
    def load_from_json(self) -> dict:
        """
        Load dataset from JSON files.

        Returns:
            dict: Loaded datasets.
        """
        # Define data files
        data_files = {
            'train': f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_train.json',
            "test": f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_test.json'
        }

        # if validation set exist, add it to data_files
        if self.has_valid:
            data_files['valid'] = f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_valid.json'
        
        # Load dataset from JSON files and post-process
        datasets = load_dataset('json', data_files=data_files)
        datasets = self._post_process(datasets)

        return datasets
    
    def load_from_dict(self, train_data, test_data, valid_data) -> dict:
        """
        Load dataset from JSON files.

        Returns:
            dict: Loaded datasets.
        """
        # Define data files
        if(len(valid_data) > 0):
            datasets = DatasetDict({
                "train": Dataset.from_list(train_data),
                "test": Dataset.from_list(test_data),
                "val": Dataset.from_list(valid_data),
            })
        else: 
            if(len(test_data) > 0):
                datasets = DatasetDict({
                    "train": Dataset.from_list(train_data),
                    "test": Dataset.from_list(test_data),
                })
            else:
                datasets = DatasetDict({
                    "train": Dataset.from_list(train_data),
                })
        datasets = self._post_process(datasets)

        return datasets
    
    def load_from_json_manual_file(self, file_path) -> dict:
        """
        Load dataset from a JSON file.

        Parameters:
            file_path (str): Path to the JSON file.

        Returns:
            dict: Loaded datasets.
        """
        data_files = {
            "test": file_path
        }
        # Load dataset from JSON file
        datasets = load_dataset('json', data_files=data_files)

        return datasets
    
    def load_llm_preds(self, path, type, split, percentage=None, target_total=None) -> Dataset:
        """
        Load rationales generated by a language model.

        Parameters:
            path (str): a directory contains rationales
            type (str): Strategy to select rationales, must be one of:
                - 'shortest': Select shortest reasoning path
                - 'all': Use all available reasoning paths
                - 'best_reward': Select highest reward reasoning path
            split (str): train or valid
            percentage (float, optional): percentage of total dataset to train (0-100)
            target_total (int, optional): maximum number of samples to select.
        
        Note: Either percentage or target_total must be provided, but not both.

        Returns:
            Dataset: A HuggingFace Dataset 
        """
        # Input validation
        if (percentage is None and target_total is None) or (percentage is not None and target_total is not None):
            raise ValueError("Exactly one of 'percentage' or 'target_total' must be provided")

        if percentage is not None and (percentage <= 0 or percentage > 100):
            raise ValueError(f"Percentage must be between 0 and 100, got {percentage}")
        
        if target_total is not None and target_total <=0:
            raise ValueError(f"Target total must be positive, got {target_total}")

        # Construct path for specific split
        split_path = os.path.join(path, split)
        all_data = []
        
        # Load all JSON files in the directory
        for file_name in glob.glob(f"{split_path}/*.json"):
            with open(file_name, 'r') as file:
                data = json.load(file)
            all_data.extend(data)

        total_samples = len(all_data)
        
        # Calculate final target total based on input method
        if percentage is not None:
            final_target = int(round((percentage / 100.0) * total_samples))
        else:
            final_target = target_total

        # Raise error if target_total would be less than 1
        if final_target < 1:
            raise ValueError(
                f"The specified percentage ({percentage}%) is too small for the dataset size ({total_samples:,} samples)."
                f"It would result in {final_target} samples. Please specify a larger percentage"
            )
        
        if self.logger:
            self.logger.info(f"Total available samples: {total_samples}")

        # Process the data using the selected method
        datasets = self._select_llm_rationales(all_data, type, final_target)
        datasets = Dataset.from_dict(datasets)

        return datasets
    
    def to_json(self, datasets) -> None:
        """
        Convert datasets to JSON format and save.

        Parameters:
            datasets (dict): Datasets to be converted to JSON and save.
        """
        # Convert each dataset in split_map to JSON
        for k,v in self.split_map.items():
            datasets[v].to_json(f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_{k}.json')

    def _post_process(self, datasets) -> None:
        """
        Perform post-processing on loaded datasets.

        Parameters:
            datasets (dict): Loaded datasets.
        
        Raises:
            NotImplementedError: if the method is not implemented.
        """
        raise NotImplementedError
    
    def _select_llm_rationales(self, outputs, type, target_total) -> dict:
        """
        Select language model rationales depending on the type.

        Parameters:
            outputs (list): List of language model outputs.
            type (str): A type to select rationales.
            target_total (int, optional): Maximum number of samples to select.
        
        Returns:
            dict: Selected datasets.
        """
        raise NotImplementedError
    
    def _select_shortest_rationales(self, data, target_total=None) -> dict:
        """
        Selects the shortest reasoning path for each question.

        Parameters:
            data (dict): Grouped data by input where each input has multiple reasoning paths.
            target_total (int, optional): Maximum number of samples to select.

        Returns:
            dict: A dictionary where each question contains only its shortest reasoning path.
        """
        selected = {}
    
        for input_key, items in data.items():
            if not items:
                continue
                
            # Sort by token count and select the shortest one
            shortest_item = min(items, key=lambda x: x['token_count'])
            selected[input_key] = [shortest_item]
        
        if target_total is not None and len(selected) > target_total:
            # Sort questions by their shortest path length
            sorted_questions = sorted(selected.items(), 
                                   key=lambda x: x[1][0]['token_count'])
            selected = dict(sorted_questions[:target_total])
            
        if self.logger:
            self.logger.info(f"Selected samples: {sum(len(samples) for samples in selected.values())}")
            self.logger.info(f"Number of questions with samples: {len(selected)}")
        
        return selected
    
    def _select_all_rationales(self, data) -> dict:
        """
        Selection method that returns all available reasoning paths.

        Parameters:
            data (dict): Grouped data by input where each input has multiple reasoning paths.

        Returns:
            dict: A dictionary where the keys represent questions and the values are all samples
        """
        if self.logger:
            total_samples = sum(len(answers) for answers in data.values())
            self.logger.info(f"Total samples: {total_samples}")
            self.logger.info(f"Number of questions: {len(data)}")

        return data
    
    def _select_best_reward_rationales(self, data, target_rationales=None) -> dict:
        """
        Selects the sample with the highest reward for each unique question.

        Parameters:
            data (dict): Grouped data by input where each input has multiple reasoning paths.
            target_rationales (int, optional): Maximum number of samples to select.

        Returns:
            dict: A dictionary where each question contains only its best reasoning path based on reward.
        """
        selected = {}

        for input_key, items in data.items():
            if not items:
                continue
                
            # Sort by reward and select the one with highest reward
            best_item = max(items, key=lambda x: x.get('reward', float('-inf')))
            selected[input_key] = [best_item]

        if target_rationales is not None and len(selected) > target_rationales:
            # Sort questions by their best reward
            sorted_questions = sorted(selected.items(), 
                                key=lambda x: x[1][0].get('reward', float('-inf')),
                                reverse=True)  # Higher reward is better
            selected = dict(sorted_questions[:target_rationales])
            
        if self.logger:
            self.logger.info(f"Selected samples: {sum(len(samples) for samples in selected.values())}")
            self.logger.info(f"Number of questions with samples: {len(selected)}")
            
            # Log reward statistics
            rewards = [item['reward'] for items in selected.values() for item in items]
            if rewards:
                self.logger.info(f"Mean reward of selected samples: {sum(rewards)/len(rewards):.4f}")
                self.logger.info(f"Max reward of selected samples: {max(rewards):.4f}")
                self.logger.info(f"Min reward of selected samples: {min(rewards):.4f}")

        return selected


class GSM8kDatasetLoader(DatasetLoader):
    """
    Class for loading and processing the GSM8k dataset.
    """

    def __init__(self, dataset_name='gsm8k', dataset_version='main', has_valid=False, split_map={'train': 'train', 'test': 'test'}) -> None:
        """
        Initialize GSM8kDatasetLoader object.

        Parameters:
            dataset_name (str): Name of the dataset
            dataset_version (str): Version of the dataset
            has_valid (bool): Indicates if the dataset has a validation set.
            split_map (dict): Mapping of splits to train and test

        """
        super().__init__(dataset_name, dataset_version, has_valid, split_map)
    
    def _post_process(self, datasets) -> None:
        """
        Perform post-processing on loaded datasets.

        Parameters:
            datasets (dict): Loaded datasets.
        
        Returns:
            dict: Processed datasets.
        """
        def prepare_input(example, idx) -> dict:
            """
            Prepare input examples.

            Parameters:
                example (dict): Dictionary containing 'question' and 'answer' keys.
                idx (int): index for the example.

            Returns:
                dict: Processed example with 'input' and 'label' keys.
            """
            question = example['question']
            answer = example['answer']

            # Compile regular expression pattern
            answer_pattern = re.compile(r"#### (\-?[0-9\.\,]+)")
            match = answer_pattern.search(answer)

            # Extract match and clean up
            if match:
                match_str = match.group(1).strip()
                match_str = match_str.replace(",", "")
                # Remove trailing zeros from decimals
                if re.match(r'^-?\d+\.\d+$', match_str):
                    match_str = match_str.rstrip('0').rstrip('.')
            else:
                raise ValueError("No match found for the pattern in the answer.")
            
            example['input'] = question
            example['label'] = match_str
            example['dataset'] = 'gsm8k'
            example['_id'] = str(idx)

            return example
        

        print(datasets)
        # Map prepare_input function to each example in the dataset
        datasets = datasets.map(prepare_input, with_indices=True)
        # Remove unnecessary columns
        datasets = datasets.remove_columns(['question', 'answer'])

        return datasets
    
    def _select_llm_rationales(self, outputs, type, target_total=None) -> dict:
        """
        Select language model rationales depending on the type.

        Parameters:
            outputs (list): List of language model outputs.
            type (str): A type to select rationales.
            target_total (int, optional): maximum number of samples to select.
        
        Returns:
            dict: Selected datasets.
        """
        datasets = {
            "input": [],
            "label": [],
            "length": [],
            "gold": [],
            "dataset": []
        }

        # Group data by input
        grouped_data = defaultdict(list)
        for item in outputs:
            grouped_data[item['_id']].append(item)

        if type == 'shortest':
            final_data = self._select_shortest_rationales(grouped_data, target_total)
        elif type == 'all':
            final_data = self._select_all_rationales(grouped_data)
        elif type == 'best_reward':
            final_data = self._select_best_reward_rationales(grouped_data, target_total)
        else:
            raise ValueError(f"Unsupported type: '{type}'. Please specify a valid type.")
        
        for items in final_data.values():
            for item in items:
                datasets['input'].append(item['input'])
                datasets['label'].append(item['rationale'])
                datasets['length'].append(item['token_count'])
                datasets['gold'].append(item['label'])
                datasets['dataset'].append(item['dataset'])
        
        return datasets


class MATHDatasetLoader(DatasetLoader):
    """
    Class for loading and processing the MATH dataset.
    """

    def __init__(self, dataset_name='math', dataset_version='main', has_valid=False, split_map={'train': 'train', 'test': 'test'}, model_name=None):
        """
        Initialize MATHDatasetLoader object.

        Parameters:
            dataset_name (str): Name of the dataset
            dataset_version (str): Version of the dataset
            has_valid (bool): Indicates if the dataset has a validation set.
            split_map (dict): Mapping of splits to train and test
            model_name (str): Model name
        """
        super().__init__(dataset_name, dataset_version, has_valid, split_map)
        self.model_name = normalize_model_name(model_name) if model_name else None
        
    def load_from_source(self):
        """
        Load dataset from HF.
        
        Raises:
            NotImplementedError: MATH dataset does not support loading from HF.
        """
        raise NotImplementedError("MATH dataset does not support loading from source.")
       
    def _post_process(self, datasets) -> None:
        """
        Perform post-processing on loaded datasets.

        Parameters:
            datasets (dict): Loaded datasets.
        
        Returns:
            dict: Processed datasets.
        """
        def prepare_input(example, idx) -> dict:
            """
            Prepare input examples.

            Parameters:
                example (dict): Dictionary containing 'problem' and 'solution' keys.
                idx (int): index for the example.

            Returns:
                dict: Processed example with 'input' and 'label' keys.
            """
            question = example['problem']
            answer = example['solution']
            
            # Select parser based on model name
            if self.model_name and is_new_parser_model(self.model_name):
                label = extract_math_answer_new(question, answer)
            else:
                label = extract_math_answer_old(question, answer)
            
            if isinstance(label, list):
                label = str(label)
            
            example['input'] = question
            example['label'] = label
            example['dataset'] = 'math'
            example['_id'] = str(idx)

            return example
        
        # Map prepare_input function to each example in the dataset
        datasets = datasets.map(prepare_input, with_indices=True)
        # Remove unnecessary columns
        datasets = datasets.remove_columns(['problem', 'solution'])

        return datasets
    
    def _select_llm_rationales(self, outputs, type, target_total=None) -> dict:
        """
        Select language model rationales depending on the type.

        Parameters:
            outputs (list): List of language model outputs.
            type (str): A type to select rationales.
            target_total (int, optional): maximum number of samples to select.
        
        Returns:
            dict: Selected datasets.
        """
        datasets = {
            "input": [],
            "label": [],
            "length": [],
            "gold": [],
            "dataset": []
        }

        # Group data by input
        grouped_data = defaultdict(list)
        for item in outputs:
            grouped_data[item['_id']].append(item)

        if type == 'shortest':
            final_data = self._select_shortest_rationales(grouped_data, target_total)
        elif type == 'all':
            final_data = self._select_all_rationales(grouped_data)
        elif type == 'best_reward':
            final_data = self._select_best_reward_rationales(grouped_data, target_total)
        else:
            raise ValueError(f"Unsupported type: '{type}'. Please specify a valid type.")
        
        for items in final_data.values():
            for item in items:
                datasets['input'].append(item['input'])
                datasets['label'].append(item['rationale'])
                datasets['length'].append(item['token_count'])
                datasets['gold'].append(item['label'])
                datasets['dataset'].append(item['dataset'])
        
        return datasets


class MMLUProDatasetLoader(DatasetLoader):
    """
    Class for loading and processing the MMLUPro dataset.
    """

    def __init__(self, dataset_name='mmlu-pro', dataset_version='default', has_valid=True, 
                 split_map={'valid': 'validation', 'test': 'test'}) -> None:
        """
        Initialize MMLUProDatasetLoader object.

        Parameters:
            dataset_name (str): Name of the dataset
            dataset_version (str): Version of the dataset
            has_valid (bool): Indicates if the dataset has a validation set.
            split_map (dict): Mapping of splits to train and test
        """
        super().__init__(dataset_name, dataset_version, has_valid, split_map)
    
    def load_from_json(self) -> dict:
        """
        Load dataset from JSON files.

        Returns:
            dict: Loaded datasets.
        """
        # Define data files - MMLU-Pro only has validation and test splits
        data_files = {
            "valid": f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_valid.json',
            "test": f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_test.json'
        }
        
        # Load dataset from JSON files and post-process
        datasets = load_dataset('json', data_files=data_files)
        datasets = self._post_process(datasets)
        
        return datasets
    
    def _post_process(self, datasets) -> None:
        """
        Perform post-processing on loaded datasets.

        Parameters:
            datasets (dict): Loaded datasets.
        
        Returns:
            dict: Processed datasets.
        """
        def prepare_input(example) -> dict:
            """
            Prepare input examples.

            Parameters:
                example (dict): Dictionary containing 'question' and 'answer' keys.
                idx (int): index for the example.

            Returns:
                dict: Processed example with 'input' and 'label' keys.
            """
            question = example['question']
            options = example['options']
            answer = example['answer']
            id = example['question_id']
            
            # Format options with letters A through J
            option_letters = "ABCDEFGHIJ"
            formatted_options = "\n".join([f"{option_letters[i]}. {option}" for i, option in enumerate(options)])
            
            # Combine question and formatted options
            formatted_input = f"{question}\n{formatted_options}"
            
            example['input'] = formatted_input
            example['label'] = answer
            example['dataset'] = 'mmlu-pro'
            example['_id'] = str(id)

            return example
        
        # Map prepare_input function to each example in the dataset
        datasets = datasets.map(prepare_input)
        # Remove unnecessary columns
        datasets = datasets.remove_columns(['question', 'answer', 'options', 'question_id'])

        return datasets
    
        
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, required=True)
    args = parser.parse_args()

    # Check if the dataset is supported
    if args.dataset == 'gsm8k':
        dataset_loader = GSM8kDatasetLoader()
    elif args.dataset == 'mmlu-pro':
        dataset_loader = MMLUProDatasetLoader()
    else:
        raise ValueError(f"Unsupported dataset: '{args.dataset}'. Please specify a valid dataset.")
    
    # Load dataset from source and save as JSON
    datasets = dataset_loader.load_from_source()
    dataset_loader.to_json(datasets)
